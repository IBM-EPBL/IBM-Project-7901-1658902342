{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "1. Required libararies are imported\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical, pad_sequences\nfrom keras.callbacks import EarlyStopping\n%matplotlib inline\n2. Read dataset and pre processing\ndf = pd.read_csv('spam.csv',delimiter=',',encoding='latin-1')\ndf.head()\nv1\tv2\tUnnamed: 2\tUnnamed: 3\tUnnamed: 4\n0\tham\tGo until jurong point, crazy.. Available only ...\tNaN\tNaN\tNaN\n1\tham\tOk lar... Joking wif u oni...\tNaN\tNaN\tNaN\n2\tspam\tFree entry in 2 a wkly comp to win FA Cup fina...\tNaN\tNaN\tNaN\n3\tham\tU dun say so early hor... U c already then say...\tNaN\tNaN\tNaN\n4\tham\tNah I don't think he goes to usf, he lives aro...\tNaN\tNaN\tNaN\ndf.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)\ndf.shape\n(5572, 2)\n#plot the ham and spam messages to understand the distribution\ndf['v1'].value_counts().plot(kind='bar')\nplt.xlabel('Label')\nplt.title('Number of ham and spam messages')\nText(0.5, 1.0, 'Number of ham and spam messages')\n\nX = df.v2\nY = df.v1\n#label encoding for Y\nle = LabelEncoder()\nY = le.fit_transform(Y)\nY = Y.reshape(-1,1)\n3. Train-test split\n#split into train and test sets\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.20)\n4. Tokenizer\nmax_words = 1000\nmax_len = 150\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = keras.utils.pad_sequences(sequences,maxlen=max_len)\n5. Add Layers(LSTM, Dense-(Hidden Layers), Output)\ninputs = Input(name='inputs',shape=[max_len])\nlayer = Embedding(max_words,50,input_length=max_len)(inputs)\nlayer = LSTM(64)(layer)\nlayer = Dense(256,name='FC1')(layer)\nlayer = Activation('relu')(layer)\nlayer = Dropout(0.5)(layer)\nlayer = Dense(1,name='out_layer')(layer)\nlayer = Activation('sigmoid')(layer)\n6. Create Model\nmodel = Model(inputs=inputs,outputs=layer)\n7. Compile the Model\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n inputs (InputLayer)         [(None, 150)]             0         \n                                                                 \n embedding (Embedding)       (None, 150, 50)           50000     \n                                                                 \n lstm (LSTM)                 (None, 64)                29440     \n                                                                 \n FC1 (Dense)                 (None, 256)               16640     \n                                                                 \n activation (Activation)     (None, 256)               0         \n                                                                 \n dropout (Dropout)           (None, 256)               0         \n                                                                 \n out_layer (Dense)           (None, 1)                 257       \n                                                                 \n activation_1 (Activation)   (None, 1)                 0         \n                                                                 \n=================================================================\nTotal params: 96,337\nTrainable params: 96,337\nNon-trainable params: 0\n_________________________________________________________________\n8.Fit the Model\nmodel.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\nEpoch 1/10\n28/28 [==============================] - 4s 148ms/step - loss: 0.0415 - accuracy: 0.9879 - val_loss: 0.0635 - val_accuracy: 0.9809\nEpoch 2/10\n28/28 [==============================] - 4s 144ms/step - loss: 0.0254 - accuracy: 0.9927 - val_loss: 0.0641 - val_accuracy: 0.9843\n9. Save the Model\nmodel.save('spam_lstm_model.h5')\n10.Test the Model\n#processing test data\ntest_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = keras.utils.pad_sequences(test_sequences,maxlen=max_len)\n#evaluation of our model\naccr = model.evaluate(test_sequences_matrix,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n35/35 [==============================] - 1s 16ms/step - loss: 0.0910 - accuracy: 0.9785\nTest set\n  Loss: 0.091\n  Accuracy: 0.978\n ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}